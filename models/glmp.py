import torch
import torch.nn as nn
from torch.optim import lr_scheduler
from torch import optim
import torch.nn.functional as F
import random
import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
import os
import json

from utils.measures import wer, moses_multi_bleu
from utils.masked_cross_entropy import *
from utils.config import *
from models.glmp_modules import *


class GLMP(nn.Module):
	def __init__(self, hidden_size, max_len, max_r, lang, path, task, lr, n_layers, dropout, unk_mask, args):
	# def __init__(self, hidden_size, lang, max_resp_len, path, task, lr, n_layers, dropout):
		super(GLMP, self).__init__()
		self.name = "GLMP"
		self.task = task
		self.input_size = lang.n_words
		self.output_size = lang.n_words
		self.hidden_size = hidden_size
		self.lang = lang
		self.lr = lr
		self.n_layers = n_layers
		self.dropout = dropout
		self.max_resp_len = max_r
		self.decoder_hop = n_layers
		self.softmax = nn.Softmax(dim=0)
		self.unk_mask = unk_mask
		self.args = args

		if path:
			if USE_CUDA:
				print("MODEL {} LOADED".format(str(path)))
				self.encoder = torch.load(str(path) + '/enc.th')
				self.extKnow = torch.load(str(path) + '/enc_kb.th')
				self.decoder = torch.load(str(path) + '/dec.th')
			else:
				print("MODEL {} LOADED".format(str(path)))
				self.encoder = torch.load(str(path) + '/enc.th', lambda storage, loc: storage)
				self.extKnow = torch.load(str(path) + '/enc_kb.th', lambda storage, loc: storage)
				self.decoder = torch.load(str(path) + '/dec.th', lambda storage, loc: storage)
		else:
			self.encoder = ContextRNN(lang.n_words, hidden_size, dropout)
			self.extKnow = ExternalKnowledge(lang.n_words, hidden_size, n_layers, dropout)
			self.decoder = LocalMemoryDecoder(self.encoder.embedding, lang, hidden_size, self.decoder_hop,
			                                  dropout)  # Generator(lang, hidden_size, dropout)

		# Initialize optimizers and criterion
		self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=lr)
		self.extKnow_optimizer = optim.Adam(self.extKnow.parameters(), lr=lr)
		self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=lr)
		self.scheduler = lr_scheduler.ReduceLROnPlateau(self.decoder_optimizer, mode='max', factor=0.5, patience=1,
		                                                min_lr=0.0001, verbose=True)
		self.criterion_bce = nn.BCELoss()
		self.reset()

		if USE_CUDA:
			self.encoder.cuda()
			self.extKnow.cuda()
			self.decoder.cuda()

	def print_loss(self):
		print_loss_avg = self.loss / self.print_every
		print_loss_g = self.loss_g / self.print_every
		print_loss_v = self.loss_v / self.print_every
		print_loss_l = self.loss_l / self.print_every
		self.print_every += 1
		return 'L:{:.2f},LE:{:.2f},LG:{:.2f},LP:{:.2f}'.format(print_loss_avg, print_loss_g, print_loss_v, print_loss_l)

	def save_model(self, dec_type):
		name_data = "KVR/" if self.task == '' else "BABI/"
		layer_info = str(self.n_layers)
		directory = 'save/GLMP-' + args["addName"] + name_data + str(self.task) + 'HDD' + str(
			self.hidden_size) + 'BSZ' + str(args['batch']) + 'DR' + str(self.dropout) + 'L' + layer_info + 'lr' + str(
			self.lr) + str(dec_type)
		if not os.path.exists(directory):
			os.makedirs(directory)
		torch.save(self.encoder, directory + '/enc.th')
		torch.save(self.extKnow, directory + '/enc_kb.th')
		torch.save(self.decoder, directory + '/dec.th')

	def reset(self):
		self.loss, self.print_every, self.loss_g, self.loss_v, self.loss_l = 0, 1, 0, 0, 0

	def _cuda(self, x):
		if USE_CUDA:
			return torch.Tensor(x).cuda()
		else:
			return torch.Tensor(x)

	def train_batch(self, data, batch_idx, clip=10, reset=0):
		if reset: self.reset()
		# Zero gradients of both optimizers
		self.encoder_optimizer.zero_grad()
		self.extKnow_optimizer.zero_grad()
		self.decoder_optimizer.zero_grad()

		# Encode and Decode
		# teacher_forcing_ratio = args['teacher_forcing_ratio']
		teacher_forcing_ratio = 0.5

		use_teacher_forcing = random.random() < teacher_forcing_ratio
		max_target_length = max(data['trg_lengths'])
		all_decoder_outputs_vocab, all_decoder_outputs_ptr, _, _, global_pointer = self.encode_and_decode(data,
		                                                                                                  max_target_length,
		                                                                                                  use_teacher_forcing,
		                                                                                                  False)

		# Loss calculation and backpropagation
		loss_g = self.criterion_bce(global_pointer, data['selector_index'])
		loss_v = masked_cross_entropy(
			all_decoder_outputs_vocab.transpose(0, 1).contiguous(),
			data['sketch_seqs'].contiguous(),
			data['trg_lengths'])
		loss_l = masked_cross_entropy(
			all_decoder_outputs_ptr.transpose(0, 1).contiguous(),
			data['ind_seqs'].contiguous(),
			data['trg_lengths'])
		loss = loss_g + loss_v + loss_l
		loss.backward()

		# Clip gradient norms
		ec = torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), clip)
		ec = torch.nn.utils.clip_grad_norm_(self.extKnow.parameters(), clip)
		dc = torch.nn.utils.clip_grad_norm_(self.decoder.parameters(), clip)

		# Update parameters with optimizers
		self.encoder_optimizer.step()
		self.extKnow_optimizer.step()
		self.decoder_optimizer.step()
		self.loss += loss.item()
		self.loss_g += loss_g.item()
		self.loss_v += loss_v.item()
		self.loss_l += loss_l.item()
		return loss.item(), loss_l.item(), loss_v.item(), loss_g.item()

	def encode_and_decode(self, data, max_target_length, use_teacher_forcing, get_decoded_words):
		# Build unknown mask for memory
		if self.unk_mask and self.decoder.training:
			story_size = data['src_seqs'].size()
			rand_mask = np.ones(story_size)
			bi_mask = np.random.binomial([np.ones((story_size[0], story_size[1]))], 1 - self.dropout)[0]
			rand_mask[:, :, 0] = rand_mask[:, :, 0] * bi_mask
			conv_rand_mask = np.ones(data['conv_seqs'].size())
			for bi in range(story_size[0]):
				start, end = data['kb_arr_lengths'][bi], data['kb_arr_lengths'][bi] + data['conv_lengths'][bi]
				conv_rand_mask[:end - start, bi, :] = rand_mask[bi, start:end, :]
			rand_mask = self._cuda(rand_mask)
			conv_rand_mask = self._cuda(conv_rand_mask)
			conv_story = data['conv_seqs'] * conv_rand_mask.long()
			story = data['src_seqs'] * rand_mask.long()
		else:
			story, conv_story = data['src_seqs'], data['conv_seqs']

		# Encode dialog history and KB to vectors
		dh_outputs, dh_hidden = self.encoder(conv_story, data['conv_lengths'])
		global_pointer, kb_readout = self.extKnow.load_memory(story, data['kb_arr_lengths'], data['conv_lengths'],
		                                                      dh_hidden, dh_outputs)
		encoded_hidden = torch.cat((dh_hidden.squeeze(0), kb_readout), dim=1)

		# Get the words that can be copy from the memory
		batch_size = len(data['src_lengths'])
		self.copy_list = []
		for elm in data['src_plain']:
			elm_temp = [word_arr[0] for word_arr in elm]
			self.copy_list.append(elm_temp)

		outputs_vocab, outputs_ptr, decoded_fine, decoded_coarse = self.decoder.forward(
			self.extKnow,
			story.size(),
			data['src_lengths'],
			self.copy_list,
			encoded_hidden,
			data['sketch_seqs'],
			max_target_length,
			batch_size,
			use_teacher_forcing,
			get_decoded_words,
			global_pointer)

		return outputs_vocab, outputs_ptr, decoded_fine, decoded_coarse, global_pointer

	def evaluate(self, dev, matric_best, early_stop=None):
		print("STARTING EVALUATION")
		# Set to not-training mode to disable dropout
		self.encoder.train(False)
		self.extKnow.train(False)
		self.decoder.train(False)

		ref, hyp = [], []
		acc, total = 0, 0
		dialog_acc_dict = {}
		F1_pred, F1_cal_pred, F1_nav_pred, F1_wet_pred = 0, 0, 0, 0
		F1_count, F1_cal_count, F1_nav_count, F1_wet_count = 0, 0, 0, 0
		pbar = tqdm(enumerate(dev), total=len(dev))
		new_precision, new_recall, new_f1_score = 0, 0, 0

		if args['dataset'] == 'kvr':
			with open('data/KVR/kvret_entities.json') as f:
				global_entity = json.load(f)
				global_entity_list = []
				for key in global_entity.keys():
					if key != 'poi':
						global_entity_list += [item.lower().replace(' ', '_') for item in global_entity[key]]
					else:
						for item in global_entity['poi']:
							global_entity_list += [item[k].lower().replace(' ', '_') for k in item.keys()]
				global_entity_list = list(set(global_entity_list))

		for j, data_dev in pbar:
			# Encode and Decode
			_, _, decoded_fine, decoded_coarse, global_pointer = self.encode_and_decode(data_dev, self.max_resp_len,
			                                                                            False, True)
			decoded_coarse = np.transpose(decoded_coarse)
			decoded_fine = np.transpose(decoded_fine)
			for bi, row in enumerate(decoded_fine):
				st = ''
				for e in row:
					if e == 'EOS':
						break
					else:
						st += e + ' '
				st_c = ''
				for e in decoded_coarse[bi]:
					if e == 'EOS':
						break
					else:
						st_c += e + ' '
				pred_sent = st.lstrip().rstrip()
				pred_sent_coarse = st_c.lstrip().rstrip()
				gold_sent = data_dev['trg_plain'][bi].lstrip().rstrip()
				ref.append(gold_sent)
				hyp.append(pred_sent)

				if args['dataset'] == 'kvr':
					# compute F1 SCORE
					single_f1, count = self.compute_prf(data_dev['entity'][bi], pred_sent.split(),
					                                    global_entity_list, data_dev['kb_plain'][bi])
					F1_pred += single_f1
					F1_count += count
					single_f1, count = self.compute_prf(data_dev['entity_cal'][bi], pred_sent.split(),
					                                    global_entity_list, data_dev['kb_plain'][bi])
					F1_cal_pred += single_f1
					F1_cal_count += count
					single_f1, count = self.compute_prf(data_dev['entity_nav'][bi], pred_sent.split(),
					                                    global_entity_list, data_dev['kb_plain'][bi])
					F1_nav_pred += single_f1
					F1_nav_count += count
					single_f1, count = self.compute_prf(data_dev['entity_wet'][bi], pred_sent.split(),
					                                    global_entity_list, data_dev['kb_plain'][bi])
					F1_wet_pred += single_f1
					F1_wet_count += count
				else:
					# compute Dialogue Accuracy Score
					current_id = data_dev['ID'][bi]
					if current_id not in dialog_acc_dict.keys():
						dialog_acc_dict[current_id] = []
					if gold_sent == pred_sent:
						dialog_acc_dict[current_id].append(1)
					else:
						dialog_acc_dict[current_id].append(0)

				# compute Per-response Accuracy Score
				total += 1
				if (gold_sent == pred_sent):
					acc += 1

				# if args['genSample']:
				# 	self.print_examples(bi, data_dev, pred_sent, pred_sent_coarse, gold_sent)

		# Set back to training mode
		self.encoder.train(True)
		self.extKnow.train(True)
		self.decoder.train(True)

		bleu_score = moses_multi_bleu(np.array(hyp), np.array(ref), lowercase=True)
		acc_score = acc / float(total)
		print("ACC SCORE:\t" + str(acc_score))

		if args['dataset'] == 'kvr':
			F1_score = F1_pred / float(F1_count)
			F1_cal_score = F1_cal_pred / float(F1_cal_count)
			F1_wet_score = F1_wet_pred / float(F1_wet_count)
			F1_nav_score = F1_nav_pred / float(F1_nav_count)
			print("F1 SCORE:\t{}".format(F1_pred / float(F1_count)))
			print("\tCAL F1:\t{}".format(F1_cal_score))
			print("\tWET F1:\t{}".format(F1_wet_score))
			print("\tNAV F1:\t{}".format(F1_nav_score))
			print("BLEU SCORE:\t" + str(bleu_score))
		else:
			dia_acc = 0
			for k in dialog_acc_dict.keys():
				if len(dialog_acc_dict[k]) == sum(dialog_acc_dict[k]):
					dia_acc += 1
			print("Dialog Accuracy:\t" + str(dia_acc * 1.0 / len(dialog_acc_dict.keys())))

		if (early_stop == 'BLEU'):
			if (bleu_score >= matric_best):
				self.save_model('BLEU-' + str(bleu_score))
				print("MODEL SAVED")
			return bleu_score
		elif (early_stop == 'ENTF1'):
			if (F1_score >= matric_best):
				self.save_model('ENTF1-{:.4f}'.format(F1_score))
				print("MODEL SAVED")
			return F1_score
		elif early_stop:
			if (acc_score >= matric_best):
				self.save_model('ACC-{:.4f}'.format(acc_score))
				print("MODEL SAVED")
			return acc_score
		else:
			return bleu_score, [F1_score, F1_cal_score, F1_wet_score, F1_nav_score]

	def compute_prf(self, gold, pred, global_entity_list, kb_plain):
		local_kb_word = [k[0] for k in kb_plain]
		TP, FP, FN = 0, 0, 0
		if len(gold) != 0:
			count = 1
			for g in gold:
				if g in pred:
					TP += 1
				else:
					FN += 1
			for p in set(pred):
				if p in global_entity_list or p in local_kb_word:
					if p not in gold:
						FP += 1
			precision = TP / float(TP + FP) if (TP + FP) != 0 else 0
			recall = TP / float(TP + FN) if (TP + FN) != 0 else 0
			F1 = 2 * precision * recall / float(precision + recall) if (precision + recall) != 0 else 0
		else:
			precision, recall, F1, count = 0, 0, 0, 0
		return F1, count

	def print_examples(self, batch_idx, data, pred_sent, pred_sent_coarse, gold_sent):
		kb_len = len(data['context_arr_plain'][batch_idx]) - data['conv_arr_lengths'][batch_idx] - 1
		print("{}: ID{} id{} ".format(data['domain'][batch_idx], data['ID'][batch_idx], data['id'][batch_idx]))
		for i in range(kb_len):
			kb_temp = [w for w in data['context_arr_plain'][batch_idx][i] if w != 'PAD']
			kb_temp = kb_temp[::-1]
			if 'poi' not in kb_temp:
				print(kb_temp)
		flag_uttr, uttr = '$u', []
		for word_idx, word_arr in enumerate(data['context_arr_plain'][batch_idx][kb_len:]):
			if word_arr[1] == flag_uttr:
				uttr.append(word_arr[0])
			else:
				print(flag_uttr, ': ', " ".join(uttr))
				flag_uttr = word_arr[1]
				uttr = [word_arr[0]]
		print('Sketch System Response : ', pred_sent_coarse)
		print('Final System Response : ', pred_sent)
		print('Gold System Response : ', gold_sent)
		print('\n')
